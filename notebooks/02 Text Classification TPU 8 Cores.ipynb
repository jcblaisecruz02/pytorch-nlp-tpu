{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU Text Classification 8 Cores",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a4fc169f51d49f489860bc7b51c6e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5718120957954ea6908b93049925d4e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f817decd86384cc9820188f6a05ff318",
              "IPY_MODEL_ce39ff759c52437dbb33dd22233c02e9"
            ]
          }
        },
        "5718120957954ea6908b93049925d4e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f817decd86384cc9820188f6a05ff318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0f5bfdb18de42a79f62d0fdd4e4c3f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 411,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 411,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0609cca7663c4c7cb238004ff6670383"
          }
        },
        "ce39ff759c52437dbb33dd22233c02e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42bac4623172425a9d25c1d574c26aef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 411/411 [00:00&lt;00:00, 1.17kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c28b654a86834827a80eeba0ee24baee"
          }
        },
        "c0f5bfdb18de42a79f62d0fdd4e4c3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0609cca7663c4c7cb238004ff6670383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42bac4623172425a9d25c1d574c26aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c28b654a86834827a80eeba0ee24baee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80b9764658ec4e4ea74c5d821adc345c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a71b6d60eaa1411b8ee15aa844fab6b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_495caf63375a46679313aaefa3d6f8bd",
              "IPY_MODEL_1b6fc70350984019959b04460ebaff43"
            ]
          }
        },
        "a71b6d60eaa1411b8ee15aa844fab6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "495caf63375a46679313aaefa3d6f8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94ed2ae7b6d04c6a8da1891088d89814",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d729274b62ed4322b1b7697c992a6fac"
          }
        },
        "1b6fc70350984019959b04460ebaff43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bce15289fc66417182f211396f1a8d76",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 1.28MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac16c72ed9c24e69887652fdfcf27ebb"
          }
        },
        "94ed2ae7b6d04c6a8da1891088d89814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d729274b62ed4322b1b7697c992a6fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bce15289fc66417182f211396f1a8d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac16c72ed9c24e69887652fdfcf27ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2707292ebf34352824370399175e233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c2ab3dd2971441769df0f5e0352e96ea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aef62d45195e4f56828c6dde88b5bab5",
              "IPY_MODEL_184c9700798e45dab1dff1922c16262f"
            ]
          }
        },
        "c2ab3dd2971441769df0f5e0352e96ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aef62d45195e4f56828c6dde88b5bab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de0cee9c07ab4ea687aa46bc99c15a74",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 263273408,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 263273408,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34de44a45a1d4ca3b0280177cdf5a02a"
          }
        },
        "184c9700798e45dab1dff1922c16262f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61858d539c2643b692f1932a68b7795a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 263M/263M [00:04&lt;00:00, 59.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ef06a458eee4be2af04a29bf4a507f2"
          }
        },
        "de0cee9c07ab4ea687aa46bc99c15a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34de44a45a1d4ca3b0280177cdf5a02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61858d539c2643b692f1932a68b7795a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ef06a458eee4be2af04a29bf4a507f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMahikqQKF2L",
        "colab_type": "text"
      },
      "source": [
        "# DistilBERT Classifier Finetuning with 8 TPU Cores\n",
        "\n",
        "*Prepared by Jan Christian Blaise Cruz*\n",
        "\n",
        "This notebook shows you how to finetune a pretrained DistilBERT model on the [Toxic Comments Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview/description) dataset using a Cloud TPU and all 8 of its cores, using PyTorch. For more information on the PyTorch XLA project, check out their [GitHub repo](https://github.com/pytorch/xla) for more tutorial notebooks and documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCNuNsUhKG3i",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B_M6KSEQT1D",
        "colab_type": "text"
      },
      "source": [
        "First, we set up PyTorch XLA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H3pPw_ZOXZl",
        "colab_type": "code",
        "outputId": "fa9dc798-ef75-4301-8954-d680d1fb09fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4264  100  4264    0     0  34666      0 --:--:-- --:--:-- --:--:-- 34666\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.5.0+cu101:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.5.0+cu101\n",
            "Uninstalling torchvision-0.6.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (352 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFlOMwfEQW7O",
        "colab_type": "text"
      },
      "source": [
        "**Note: Make sure to upload your ```kaggle.json``` file in order to download the data using the API.** More information about the API can be found [here](https://www.kaggle.com/docs/api).\n",
        "\n",
        "We'll download the 2018 Toxic Comments Classification Challenge dataset from Kaggle, unzip the files, then install the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLR94Zj9OeSw",
        "colab_type": "code",
        "outputId": "533685a6-d630-4724-9b70-a4484b69fb54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set up the API\n",
        "# Make sure to upload your kaggle.json file first!\n",
        "!mkdir ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the competition data\n",
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "!unzip train.csv && unzip test.csv && unzip sample_submission.csv.zip && unzip test_labels.csv.zip\n",
        "\n",
        "# Download packages\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 73% 17.0M/23.4M [00:00<00:00, 56.6MB/s]\n",
            "100% 23.4M/23.4M [00:00<00:00, 78.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 194MB/s]\n",
            "Downloading test_labels.csv.zip to /content\n",
            "  0% 0.00/1.46M [00:00<?, ?B/s]\n",
            "100% 1.46M/1.46M [00:00<00:00, 199MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 65% 17.0M/26.3M [00:00<00:00, 176MB/s]\n",
            "100% 26.3M/26.3M [00:00<00:00, 168MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 3.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 18.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=60ee03cdb3cef59dced1711981523506c8556e0f4ceb34ca6720a3e2d8b8479b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m7cu5J5Qp1f",
        "colab_type": "text"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz8ClfhzQrMP",
        "colab_type": "text"
      },
      "source": [
        "First, some imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1h7CM4NOj0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as datautils\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuxHQ7uvQuJd",
        "colab_type": "text"
      },
      "source": [
        "For this example run, we'll use DistilBERT to finetune a toxic comment classifier. \n",
        "\n",
        "You are free to change this to any other pretrained checkpoint, but be sure to tweak the hyperparameters later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEXAyQqxJe3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained = 'distilbert-base-cased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRk2hK12Q4G7",
        "colab_type": "text"
      },
      "source": [
        "Here's a function to encode all the text in the dataset into an array of indices from the tokenizer's vocabulary. We'll also load the pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsIgeNLqOxv1",
        "colab_type": "code",
        "outputId": "4d8591cb-f3cd-4dc5-83d0-be37fc3c1c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "9a4fc169f51d49f489860bc7b51c6e56",
            "5718120957954ea6908b93049925d4e6",
            "f817decd86384cc9820188f6a05ff318",
            "ce39ff759c52437dbb33dd22233c02e9",
            "c0f5bfdb18de42a79f62d0fdd4e4c3f6",
            "0609cca7663c4c7cb238004ff6670383",
            "42bac4623172425a9d25c1d574c26aef",
            "c28b654a86834827a80eeba0ee24baee",
            "80b9764658ec4e4ea74c5d821adc345c",
            "a71b6d60eaa1411b8ee15aa844fab6b8",
            "495caf63375a46679313aaefa3d6f8bd",
            "1b6fc70350984019959b04460ebaff43",
            "94ed2ae7b6d04c6a8da1891088d89814",
            "d729274b62ed4322b1b7697c992a6fac",
            "bce15289fc66417182f211396f1a8d76",
            "ac16c72ed9c24e69887652fdfcf27ebb"
          ]
        }
      },
      "source": [
        "# Fast encoding function\n",
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a4fc169f51d49f489860bc7b51c6e56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80b9764658ec4e4ea74c5d821adc345c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrKZRC4RFzT",
        "colab_type": "text"
      },
      "source": [
        "Load the training dataset. Encode the text and extract the gold labels (remember we have six classes to predict). Split these into training and validation sets. We'll convert the labels into float tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ed-KRgzO0k0",
        "colab_type": "code",
        "outputId": "e5cd38ef-ab21-4e9e-8d37-c67174dad2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Encode the dataset\n",
        "s = time.time()\n",
        "text = regular_encode(list(df['comment_text']), tokenizer)\n",
        "labels = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "print(\"Elapsed: {:.2f}s\".format(time.time() - s))\n",
        "\n",
        "# Split into training and testing\n",
        "tr_sz = int(len(text) * 0.7)\n",
        "X_train, y_train = torch.tensor(text[:tr_sz]), torch.tensor(labels[:tr_sz]).float()\n",
        "X_valid, y_valid = torch.tensor(text[tr_sz:]), torch.tensor(labels[tr_sz:]).float()\n",
        "\n",
        "# Produce datasets\n",
        "train_set = datautils.TensorDataset(X_train, y_train)\n",
        "valid_set = datautils.TensorDataset(X_valid, y_valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 194.24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGDl1RqiRXkd",
        "colab_type": "text"
      },
      "source": [
        "The metric for the competition is \"mean column-wise ROC AUC,\" which isn't directly implemented so we'll implement it here. We're essentially just computing the ROC AUC for each of the six classes, then getting their average. \n",
        "\n",
        "We'll try to catch ```ValueError``` exceptions here which will happen if the model assigned (predicted) all the data into one class -- which is very rare -- but still a possibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqL5iTmvRgeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roc_auc(preds, actuals):\n",
        "    scores = []\n",
        "    for i in range(actuals.shape[1]):\n",
        "        try: score = roc_auc_score(actuals[:, i], preds[:, i])\n",
        "        except ValueError: score = 0 # In case only one class is present\n",
        "        scores.append(score)\n",
        "    return np.array(scores).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yBtpquJSCT2",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8JlQgOqSFZF",
        "colab_type": "text"
      },
      "source": [
        "We'll now write the function that will be mapped to all 8 TPU cores. For more granular information on how PyTorch XLA maps to specific cores, check out [this notebook](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb).\n",
        "\n",
        "This function will be long, so we'll comment in everything that you'll need to know."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwlmFkBJPHh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    # Set the seed and obtain an XLA device\n",
        "    torch.manual_seed(flags['seed'])\n",
        "    device = xm.xla_device()\n",
        "    print(\"Process\", index, \"obtained, using device:\", xm.xla_real_devices([str(device)])[0]) \n",
        "\n",
        "    # Produce distributed samplers\n",
        "    train_sampler = datautils.distributed.DistributedSampler(\n",
        "        train_set, \n",
        "        num_replicas=xm.xrt_world_size(), \n",
        "        rank=xm.get_ordinal(), \n",
        "        shuffle=True\n",
        "    )\n",
        "    valid_sampler = datautils.distributed.DistributedSampler(\n",
        "        valid_set, \n",
        "        num_replicas=xm.xrt_world_size(), \n",
        "        rank=xm.get_ordinal(), \n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = datautils.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=flags['batch_size'], \n",
        "        sampler=train_sampler, \n",
        "        num_workers=flags['num_workers'],\n",
        "        drop_last=True\n",
        "    )\n",
        "    valid_loader = datautils.DataLoader(\n",
        "        valid_set,\n",
        "        batch_size=flags['batch_size'], \n",
        "        sampler=valid_sampler, \n",
        "        num_workers=flags['num_workers'],\n",
        "        drop_last=True,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # This ensures that the pretrained weights will only be\n",
        "    # downloaded once (c/o the master process). It also makes\n",
        "    # sure that the other processes don't attempt to load the\n",
        "    # weights when downloading isn't finished yet.\n",
        "    if not xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    # Configure the model\n",
        "    config = AutoConfig.from_pretrained(flags['pretrained'], num_labels=flags['num_labels'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(flags['pretrained'], config=config).to(device)\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=flags['learning_rate'])\n",
        "\n",
        "    xm.master_print(\"\\nNumber of training batches: {}\".format(len(train_loader)))\n",
        "    xm.master_print(\"Number of evaluation batches: {}\\n\".format(len(valid_loader)))\n",
        "\n",
        "    # Train Model\n",
        "    model.train()\n",
        "    train_start = time.time()\n",
        "    \n",
        "    for e in range(1, flags['num_epochs'] + 1):\n",
        "        xm.master_print(\"=\" * 27 + \"Epoch {} of {}\".format(e, flags['num_epochs']) + \"=\" * 27)\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "        for i, batch in enumerate(para_train_loader):\n",
        "            x, y = batch\n",
        "            out = model(x)[0]\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            if i % flags['print_every'] == 0:\n",
        "                xm.master_print('[TRAIN] Iteration {:4} | Loss {:.4f} | Time Elapsed {:.2f} seconds'.format(i, loss.item(),time.time() - train_start))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "    xm.master_print('\\nFinished training {} epochs in {:.2f} seconds.\\n'.format(flags['num_epochs'], time.time() - train_start))\n",
        "\n",
        "    # Evaluate Model\n",
        "    model.eval()\n",
        "    valid_start = time.time()\n",
        "    preds, actuals = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        xm.master_print('=' * 28 + 'Validation' + '=' * 28)\n",
        "        para_valid_loader = pl.ParallelLoader(valid_loader, [device]).per_device_loader(device)\n",
        "        for i, batch in enumerate(para_valid_loader):\n",
        "            x, y = batch\n",
        "            out = model(x)[0]\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            # Keep track of all outputs and gold labels\n",
        "            actuals.extend(y.cpu().numpy().tolist())\n",
        "            preds.extend(out.cpu().detach().numpy().tolist())\n",
        "\n",
        "            if i % flags['print_every'] == 0:\n",
        "                xm.master_print('[VALID] Iteration {:4} | Loss {:.4f} | Time Elapsed {:.2f} seconds'.format(i, loss.item(),time.time() - train_start))\n",
        "\n",
        "    preds, actuals = np.array(preds), np.array(actuals)\n",
        "    valid_auroc = roc_auc(preds, actuals)\n",
        "    xm.master_print('\\nFinished evaluation in {:.2f} seconds. Validation AUROC: {:.4f}\\n'.format(time.time() - valid_start, valid_auroc))\n",
        "\n",
        "    # Save the model\n",
        "    xm.save(model.state_dict(), flags['savedir'] + '/' + flags['modelpath'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKnT0y2DS4fw",
        "colab_type": "text"
      },
      "source": [
        "We'll set the hyperparamters into a dictionary we call ```flags```. If you're coding this into a script instead, this can conveniently come from command line arguments. A word on the batch size: note that we're *technically* doing batch size 128 here, since we're training 16 batches on 8 cores at once.\n",
        "\n",
        "We'll also create a directory to store our model weights (and other things later). Then we start the distributed process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwPkT3RMRpo2",
        "colab_type": "code",
        "outputId": "128b3d0b-e29e-4a40-a581-d78cecf930d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849,
          "referenced_widgets": [
            "c2707292ebf34352824370399175e233",
            "c2ab3dd2971441769df0f5e0352e96ea",
            "aef62d45195e4f56828c6dde88b5bab5",
            "184c9700798e45dab1dff1922c16262f",
            "de0cee9c07ab4ea687aa46bc99c15a74",
            "34de44a45a1d4ca3b0280177cdf5a02a",
            "61858d539c2643b692f1932a68b7795a",
            "3ef06a458eee4be2af04a29bf4a507f2"
          ]
        }
      },
      "source": [
        "# Set flags\n",
        "flags = {\n",
        "    'batch_size': 16,\n",
        "    'num_workers': 8,\n",
        "    'num_epochs': 3,\n",
        "    'seed': 42,\n",
        "    'num_labels': 6,\n",
        "    'pretrained': pretrained,\n",
        "    'savedir': 'training_dir',\n",
        "    'modelpath': 'model.bin',\n",
        "    'learning_rate': 1e-5,\n",
        "    'print_every': 150\n",
        "}\n",
        "\n",
        "# Start the process\n",
        "if flags['savedir'] not in os.listdir('.'): os.mkdir(flags['savedir'])\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 0 obtained, using device: TPU:0\n",
            "Process 4 obtained, using device: TPU:4\n",
            "Process 6 obtained, using device: TPU:6\n",
            "Process 5 obtained, using device: TPU:5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2707292ebf34352824370399175e233",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Process 3 obtained, using device: TPU:3\n",
            "Process 2 obtained, using device: TPU:2\n",
            "Process 7 obtained, using device: TPU:7\n",
            "Process 1 obtained, using device: TPU:1\n",
            "\n",
            "\n",
            "Number of training batches: 872\n",
            "Number of evaluation batches: 374\n",
            "\n",
            "===========================Epoch 1 of 3===========================\n",
            "[TRAIN] Iteration    0 | Loss 0.7006 | Time Elapsed 4.41 seconds\n",
            "[TRAIN] Iteration  150 | Loss 0.0825 | Time Elapsed 98.67 seconds\n",
            "[TRAIN] Iteration  300 | Loss 0.1211 | Time Elapsed 153.47 seconds\n",
            "[TRAIN] Iteration  450 | Loss 0.1496 | Time Elapsed 208.38 seconds\n",
            "[TRAIN] Iteration  600 | Loss 0.0523 | Time Elapsed 263.86 seconds\n",
            "[TRAIN] Iteration  750 | Loss 0.0276 | Time Elapsed 319.04 seconds\n",
            "===========================Epoch 2 of 3===========================\n",
            "[TRAIN] Iteration    0 | Loss 0.0946 | Time Elapsed 366.67 seconds\n",
            "[TRAIN] Iteration  150 | Loss 0.0289 | Time Elapsed 423.45 seconds\n",
            "[TRAIN] Iteration  300 | Loss 0.0364 | Time Elapsed 479.03 seconds\n",
            "[TRAIN] Iteration  450 | Loss 0.0785 | Time Elapsed 534.68 seconds\n",
            "[TRAIN] Iteration  600 | Loss 0.0576 | Time Elapsed 593.00 seconds\n",
            "[TRAIN] Iteration  750 | Loss 0.0168 | Time Elapsed 648.79 seconds\n",
            "===========================Epoch 3 of 3===========================\n",
            "[TRAIN] Iteration    0 | Loss 0.0861 | Time Elapsed 697.14 seconds\n",
            "[TRAIN] Iteration  150 | Loss 0.0270 | Time Elapsed 753.59 seconds\n",
            "[TRAIN] Iteration  300 | Loss 0.0323 | Time Elapsed 809.44 seconds\n",
            "[TRAIN] Iteration  450 | Loss 0.0666 | Time Elapsed 865.38 seconds\n",
            "[TRAIN] Iteration  600 | Loss 0.0401 | Time Elapsed 921.27 seconds\n",
            "[TRAIN] Iteration  750 | Loss 0.0122 | Time Elapsed 977.26 seconds\n",
            "\n",
            "Finished training 3 epochs in 1023.42 seconds.\n",
            "\n",
            "============================Validation============================\n",
            "[VALID] Iteration    0 | Loss 0.0582 | Time Elapsed 1033.13 seconds\n",
            "[VALID] Iteration  150 | Loss 0.0009 | Time Elapsed 1056.29 seconds\n",
            "[VALID] Iteration  300 | Loss 0.0129 | Time Elapsed 1075.24 seconds\n",
            "\n",
            "Finished evaluation in 61.76 seconds. Validation AUROC: 0.9874\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0wh2HN5j5et",
        "colab_type": "text"
      },
      "source": [
        "TPUs are very fast! We finished finetuning and validation in about ~18 minutes all in all.\n",
        "\n",
        "In comparison, finetuning for this dataset on a P100 GPU (batch size 32, all other hyperparameters the same) takes about ~2 hours 21 minutes for the full three epochs. That's a very big difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnwgk42GeOP3",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIz5ifUQTK3v",
        "colab_type": "text"
      },
      "source": [
        "For inferencing, we'd want to make sure that each prediction will be paired with the correct id from the dataset (so kaggle can score our predictions).  We'll subclass ```torch.utils.data.Dataset``` to allow non tensors to be included in a tensor dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YoyOn6TdSLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestDataset(datautils.Dataset):\n",
        "    def __init__(self, text, ids):\n",
        "        self.text = text\n",
        "        self.ids = ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ix_text = self.text[idx]\n",
        "        ix_id = self.ids[idx]\n",
        "        \n",
        "        return ix_text, ix_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7cM1oQpUZ5C",
        "colab_type": "text"
      },
      "source": [
        "We'll read the test set, encode the text, and extract the corresponding list of IDs. We'll construct a dataset and a dataloader from this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYJC059vR2dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "text = regular_encode(list(df['comment_text']), tokenizer)\n",
        "ids = list(df['id'])\n",
        "\n",
        "# Produce a test set and loader\n",
        "test_set = TestDataset(text, ids)\n",
        "test_loader = datautils.DataLoader(test_set, batch_size=16, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqINBoevUh_h",
        "colab_type": "text"
      },
      "source": [
        "We'll write another mapping function for distributed inferencing. The details are mostly the same. The only difference is that at the end of inferencing, each process will save its predictions + their corresponding IDs in the folder that we created earlier. We'll collate all these predictions later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEyPCl6cdSNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    # Set the seed and obtain an XLA device\n",
        "    torch.manual_seed(flags['seed'])\n",
        "    device = xm.xla_device()\n",
        "    print(\"Process\", index, \"obtained, using device:\", xm.xla_real_devices([str(device)])[0])\n",
        "\n",
        "    # Produce a distributed sampler and a data loader\n",
        "    test_sampler = datautils.distributed.DistributedSampler(\n",
        "        test_set,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = datautils.DataLoader(\n",
        "        test_set,\n",
        "        batch_size=flags['batch_size'],\n",
        "        sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=flags['num_workers']\n",
        "    )\n",
        "\n",
        "    # Configure the model and load the checkpoint\n",
        "    config = AutoConfig.from_pretrained(flags['pretrained'], num_labels=flags['num_labels'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(flags['pretrained'], config=config).to(device)\n",
        "    model.load_state_dict(torch.load(flags['savedir'] + '/' + flags['modelpath']))\n",
        "\n",
        "    xm.master_print(\"\\nNumber of testing batches: {}\\n\".format(len(test_loader)))\n",
        "\n",
        "    # Run inferencing\n",
        "    model.eval()\n",
        "    preds, ids = [], []\n",
        "    test_start = time.time()\n",
        "\n",
        "    xm.master_print('=' * 25 + 'Inference' + '=' * 25)\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        x, idx = batch\n",
        "        x = x.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = torch.sigmoid(model(x)[0])\n",
        "            preds.extend(out.cpu().detach().numpy().tolist())\n",
        "            ids.extend(idx)\n",
        "        if i % flags['print_every'] == 0: \n",
        "            xm.master_print('Inferencing on step {:4} | Time elapsed: {:.2f} seconds'.format(i, time.time() - test_start))\n",
        "    preds = np.array(preds)\n",
        "\n",
        "    # Save the predictions and associated IDs into a temporary file\n",
        "    with open('{}/preds_{}.pt'.format(flags['savedir'], xm.xla_real_devices([str(device)])[0]), 'wb') as f:\n",
        "        torch.save([ids, preds], f)\n",
        "\n",
        "    xm.master_print('\\nFinished inferencing in {:.2f} seconds.\\n'.format(time.time() - test_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEHuVe4jU2EN",
        "colab_type": "text"
      },
      "source": [
        "Then we spawn the processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP_BfLzae3St",
        "colab_type": "code",
        "outputId": "f23aa2bd-dfa3-4c71-836c-925b7fe53312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "# Start the processes\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 0 obtained, using device: TPU:0\n",
            "Process 4 obtained, using device: TPU:4\n",
            "Process 3 obtained, using device: TPU:3\n",
            "Process 7 obtained, using device: TPU:7\n",
            "Process 5 obtained, using device: TPU:5\n",
            "Process 6 obtained, using device: TPU:6\n",
            "Process 1 obtained, using device: TPU:1\n",
            "Process 2 obtained, using device: TPU:2\n",
            "\n",
            "Number of testing batches: 1197\n",
            "\n",
            "=========================Inference=========================\n",
            "Inferencing on step    0 | Time elapsed: 5.58 seconds\n",
            "Inferencing on step  150 | Time elapsed: 27.76 seconds\n",
            "Inferencing on step  300 | Time elapsed: 42.71 seconds\n",
            "Inferencing on step  450 | Time elapsed: 58.69 seconds\n",
            "Inferencing on step  600 | Time elapsed: 73.84 seconds\n",
            "Inferencing on step  750 | Time elapsed: 88.64 seconds\n",
            "Inferencing on step  900 | Time elapsed: 104.22 seconds\n",
            "Inferencing on step 1050 | Time elapsed: 120.16 seconds\n",
            "\n",
            "Finished inferencing in 139.38 seconds.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rKBTvFNU-WW",
        "colab_type": "text"
      },
      "source": [
        "Afterwards, we'll load all the prediction files and collate them into a Pandas DataFrame. There **will** be a number of duplicate ids, which is a consequence of the distributed strategy. We'll just keep the first that Pandas will see and drop the rest. The difference between the duplicates is very *very* miniscule, considering that they were inferred from the same finetuned weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvZlgjUOe3XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all prediction files\n",
        "labellist = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "all_ids, all_preds = [], []\n",
        "for i in range(8):\n",
        "    with open('{}/preds_TPU:{}.pt'.format(flags['savedir'], i), 'rb') as f:\n",
        "        idx, preds = torch.load(f)\n",
        "        all_ids.extend(idx)\n",
        "        all_preds.extend(preds)\n",
        "preds = np.array(all_preds)\n",
        "\n",
        "# Combine and remove duplicates\n",
        "submission = pd.DataFrame(data={'id': all_ids})\n",
        "for label in labellist:\n",
        "    submission[label] = 0\n",
        "submission[labellist] = preds\n",
        "submission.drop_duplicates(keep='first', subset='id', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpLkr4lrVeZg",
        "colab_type": "text"
      },
      "source": [
        "We then check if the length of our predictions is the same with the length of the test set, then save the predictions to a submission file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3SBCGWmohp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save CSV\n",
        "assert submission.shape[0] == df.shape[0]\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpbtqL24pOgt",
        "colab_type": "text"
      },
      "source": [
        "Sending our submission file to Kaggle got me a score of 0.97847 on the public leaderboard, which is 0.01054 away from the top score! That's a pretty good result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9puoqEeceGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}